{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T02:22:42.314360Z",
     "start_time": "2019-03-04T02:22:42.233109Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tools import load_data, save_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T02:22:45.157828Z",
     "start_time": "2019-03-04T02:22:45.129101Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!! Do not modify !!\n",
    "\"\"\"\n",
    "def dumb_featurize(text):\n",
    "    feats = {}\n",
    "    words = text.split(\" \")\n",
    "\n",
    "    for word in words:\n",
    "        if word == \"love\" or word == \"like\" or word == \"best\":\n",
    "            feats[\"contains_positive_word\"] = 1\n",
    "        if word == \"hate\" or word == \"dislike\" or word == \"worst\" or word == \"awful\":\n",
    "            feats[\"contains_negative_word\"] = 1\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T02:22:46.340973Z",
     "start_time": "2019-03-04T02:22:46.313272Z"
    }
   },
   "outputs": [],
   "source": [
    "def better_featurize(text):\n",
    "    raise NotImplementedError\n",
    "    \"\"\"\n",
    "    !! Do not work on this yet, work on the model and come back later !!\n",
    "    \n",
    "    Write your own code below\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T02:36:01.246736Z",
     "start_time": "2019-03-04T02:36:01.215699Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import dok_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class SentimentClassifier:\n",
    "    def __init__(self, feature_method=dumb_featurize, min_feature_ct=1, L2_reg=1.0):\n",
    "        \"\"\"\n",
    "        :param feature_method: featurize function\n",
    "        :param min_feature_count: int, ignore the features that appear less than this number to avoid overfitting\n",
    "        \"\"\"\n",
    "        self.feature_vocab = {}\n",
    "        self.feature_method = feature_method\n",
    "        self.min_feature_ct = min_feature_ct\n",
    "        self.L2_reg = L2_reg\n",
    "\n",
    "    def featurize(self, X):\n",
    "        \"\"\"\n",
    "        # Featurize input text\n",
    "\n",
    "        :param X: list of texts\n",
    "        :return: list of featurized vectors\n",
    "        \"\"\"\n",
    "        featurized_data = []\n",
    "        for text in X:\n",
    "            feats = self.feature_method(text)\n",
    "            featurized_data.append(feats)\n",
    "        return featurized_data\n",
    "\n",
    "    def pipeline(self, X, training=False):\n",
    "        \"\"\"\n",
    "        Data processing pipeline to translate raw data input into sparse vectors\n",
    "        :param X: featurized input\n",
    "        :return: 2d sparse vectors\n",
    "        \n",
    "        Implement the pipeline method that translate the dictionary like feature vectors into homogeneous numerical\n",
    "        vectors, for example:\n",
    "        [{\"fea1\": 1, \"fea2\": 2}, \n",
    "         {\"fea2\": 2, \"fea3\": 3}] \n",
    "         --> \n",
    "         [[1, 2, 0], \n",
    "          [0, 2, 3]]\n",
    "          \n",
    "        Hints:\n",
    "        1. How can you know the length of the feature vector?\n",
    "        2. When should you use sparse matrix?\n",
    "        3. Have you treated non-seen features properly?\n",
    "        4. Should you treat training and testing data differently?\n",
    "        \"\"\"\n",
    "        # Have to build feature_vocab during training\n",
    "        if training:\n",
    "            raise NotImplementedError\n",
    "         \n",
    "        # Translate raw texts into vectors\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self.pipeline(self.featurize(X), training=True)\n",
    "\n",
    "        D, F = X.shape\n",
    "        self.model = LogisticRegression(C=self.L2_reg)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.pipeline(self.featurize(X))\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        X = self.pipeline(self.featurize(X))\n",
    "        return self.model.score(X, y)\n",
    "\n",
    "    # Write learned parameters to file\n",
    "    def save_weights(self, filename='weights.csv'):\n",
    "        weights = [[\"__intercept__\", self.model.intercept_[0]]]\n",
    "        for feat, idx in self.feature_vocab.items():\n",
    "            weights.append([feat, self.model.coef_[0][idx]])\n",
    "        \n",
    "        weights = pd.DataFrame(weights)\n",
    "        weights.to_csv(filename, header=False, index=False)\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T02:36:02.841682Z",
     "start_time": "2019-03-04T02:36:02.812608Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run this to test your model implementation\n",
    "\"\"\"\n",
    "\n",
    "cls = SentimentClassifier()\n",
    "X_train = [{\"fea1\": 1, \"fea2\": 2}, {\"fea2\": 2, \"fea3\": 3}]\n",
    "\n",
    "X = cls.pipeline(X_train, True)\n",
    "assert X.shape[0] == 2 and X.shape[1] >= 3, \"Fail to vectorize training features\"\n",
    "\n",
    "X_test = [{\"fea1\": 1, \"fea2\": 2}, {\"fea2\": 2, \"fea3\": 3}]\n",
    "X = cls.pipeline(X_test)\n",
    "assert X.shape[0] == 2 and X.shape[1] >= 3, \"Fail to vectorize testing features\"\n",
    "\n",
    "X_test = [{\"fea1\": 1, \"fea2\": 2}, {\"fea2\": 2, \"fea4\": 3}]\n",
    "try:\n",
    "    X = cls.pipeline(X_test)\n",
    "    assert X.shape[0] == 2 and X.shape[1] >= 3\n",
    "except:\n",
    "    print(\"Fail to treat un-seen features\")\n",
    "    raise Exception\n",
    "    \n",
    "print(\"Success!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T02:36:14.338290Z",
     "start_time": "2019-03-04T02:36:13.670195Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run this cell to test your model performance\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_data(\"train.txt\")\n",
    "X, y = data.text, data.target\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3)\n",
    "cls = SentimentClassifier(feature_method=dumb_featurize)\n",
    "cls = cls.fit(X_train, y_train)\n",
    "print(\"Training set accuracy: \", cls.score(X_train, y_train))\n",
    "print(\"Dev set accuracy: \", cls.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T02:10:36.091653Z",
     "start_time": "2019-03-04T02:10:35.890808Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run this cell to save weights and the prediction\n",
    "\"\"\"\n",
    "weights = cls.save_weights()\n",
    "\n",
    "X_test = load_data(\"test.txt\").text\n",
    "save_prediction(cls.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Use different learning methods\n",
    "\n",
    "God job reaching this point! So far you have explored many different ways of doing feature engineering, but how about the learning method? In the previous implementation Logistic Regression was used. Now you can try to use different learning methods.\n",
    "\n",
    "hint: inherit the previous model and overwrite the `fit` method "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
